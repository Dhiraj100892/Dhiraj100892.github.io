<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html class="gr__cs_cmu_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
        
        <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
        <meta name="keywords" content="Dhiraj, Gandhi, Robot, Learning, Computer Vision, Big Data">
        <meta name="description" content="About Dhiraj Gandhi">
        <meta name="author" content="Dhiraj Gandhi">
        <meta name="robots" content="index">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <style type="text/css">
            /* Stolen from Jon Barron via Aayush */
            a {
            color: #1772d0;
            text-decoration:none;
            }
            a:focus, a:hover {
            color: #f09228;
            text-decoration:none;
            }
            body,td,th {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
            }
            strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 13px
            }
            heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
            }
        </style>
        <link rel="icon" type="image/png" href="http://www.ri.cmu.edu/images/site_images/favicon.png">
        <title>Dhiraj Gandhi</title>
        <link href="./dhiraj gandhi_files/css" rel="stylesheet" type="text/css">
    </head>
    <body data-gr-c-s-loaded="true">
        <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
        <tbody>
            <tr>
                <td>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="67%" valign="middle">
                                    <p align="center"><font size="6">Dhiraj Gandhi</font>
                                    </p>
                                    <div align="justify">
                                        <p>I am currently working as Research Engineer at Facebook AI Research(FAIR), Pittsburgh lab. I graduated from Masters program at <a href="http://www.ri.cmu.edu/">The Robotics Institute</a>, <a href="http://www.cmu.edu/">Carnegie Mellon University (CMU)</a> advised by Prof.<a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a> in July 2019.   
                                        </p>
                                        <p>I have completed my bachelors in Mechanical engineering from the <a href="http://www.iitg.ac.in/">Indian Institute of Technology (IIT) Guwahati</a> in 2015. Before joining CMU as a master student, I had spent around half a year as an engineer at <a href="http://www.hitechroboticsystemz.com/">The Hitech Robotic Systemz</a> and two year & half as a research associate at CMU with Prof. Abhinav Gupta.
                                        </p>
                                    </div>
                                    <p align="center">
                                        <style type="”text/css”">
                                            p span.displaynone { display:none; }
                                        </style>
                                    </p><p align="center">Email:dhirajgandhi[AT]<span class="”displaynone”"></span>fb.com</p>
				    <p></p>
                                    <!--. <a href="./CV.pdf">CV</a>--> 
                                    <p></p>
                                </td>
                                <td width="33%"><img width="300" src="./dhiraj gandhi_files/dhiraj.jpg"></td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Press Coverage</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="100%">
                                    <p>Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias</p>
                                        <a href="https://www.wired.com/story/robots-are-renting-airbnbs-to-get-a-better-grip/"><img src="./dhiraj gandhi_files/wired_logo.svg" height="100" width="180" style="border-style: none"></a>
                                    <p>Learning to Fly by Crashing</p>
                                        <a href="http://spectrum.ieee.org/automaton/robotics/drones/drone-uses-ai-and-11500-crashes-to-learn-how-to-fly"><img src="https://fs25.formsite.com/ieeevcep/images/IEEE-Spectrum.Horizontal.jpg" height="60" width="180" style="border-style: none"></a>
                                        <a href="http://www.popularmechanics.com/flight/drones/a26490/drones-learned-to-fly-by-crashing/"><img src="http://pop.h-cdn.co/assets/popularmechanics/20170509153642/images/apple-touch-icon.png" height="60" style="border-style: none"></a>
                                        <a href="https://www.digitaltrends.com/cool-tech/crashing-drones-teach-fly-better/"><img src="./dhiraj gandhi_files/digital_trend.jpeg" height="60" style="border-style: none"></a>
                                        <a href="https://news.ycombinator.com/item?id=14316917"><img src="https://d1wgc1sljrdxgx.cloudfront.net/blog/Developer+Communities/hacker-news-logo.jpg" height="60" style="border-style: none"></a>
                                        <a href="https://skytango.com/drone-learns-to-fly-autonomously-by-crashing-11500-times/"><img src="https://skytango.com/wp-content/uploads/2016/11/skytango-logo-circle-r.png" width="100" height="50" style="border-style: none"></a>
                                        <a href="https://www.reddit.com/r/MachineLearning/comments/66h9as/r170405588_learning_to_fly_by_crashing/"><img src="https://assets.change.org/photos/3/gb/gw/SdgbgWIPpyJFIRp-800x450-noPad.jpg?1513898229" height="60" style="border-style: none"></a>
                                   <p>Curious Robot</p>
				                        <a href="https://techcrunch.com/2017/06/28/teaching-robots-to-learn-about-the-world-through-touch/?ncid=mobilerecirc_recent"><img src="https://s0.wp.com/wp-content/themes/vip/techcrunch-2013/assets/images/logo.svg" height="60" style="border-style: none"></a>
                                        <a href="http://qz.com/675276/ais-are-starting-to-learn-like-human-babies-by-grasping-and-poking-objects/"><img src="https://app.qz.com/img/qz_og_img.png" height="60" style="border-style: none"></a>
                                 </td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Research and Selected Projects</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

            <!-- ICLR2020 -->
			<tr>
                <td width="25%" align="center">
                    <a href="https://openreview.net/pdf?id=HklXn1BKDH" target="_blank"><img class="centered-and-cropped"  src="./dhiraj gandhi_files/iclr_2020_teaser.gif" width="100%" alt="GIF" /></a>
                </td>
                <td width="75%" valign="top">
                    <p>
                        <a href="http://www.cs.cmu.edu/~dchaplot/projects/neural-slam.html" target="_blank">
                            <heading>Leaning to Explore using Active Neural SLAM</heading>
                        </a>
                        <br>
                        <a href="https://http//devendrachaplot.github.io/" target="_blank">Devendra Chaplot</a> , <strong>Dhiraj Gandhi</strong>, <a href="http://saurabhg.web.illinois.edu/" target="_blank">Saurabh Gupta, <a href="http://www.cs.cmu.edu/~abhinavg/" target="_blank">Abhinav Gupta</a>, <a href="https://www.cs.cmu.edu/~rsalakhu/" target="_blank">Ruslan Salakhutdinov</a></a>
                        <br>
                        <strong>ICLR 2020</strong><br>
                        <a href="https://openreview.net/pdf?id=HklXn1BKDH" target="_blank">Paper</a>, <a href="http://www.cs.cmu.edu/~dchaplot/projects/neural-slam.html" target="_blank">Project page</a>
                    </p>
                    <p align="justify"> This work presents a modular and hierarchical approach to learn policies for exploring 3D environments, called Active Neural SLAM. Our approach leverages the strengths of both classical and learning-based methods, by using analytical path planners with learned SLAM module, and global and local policies.The proposed model can also be easily transferred to the PointGoal task and was the winning entry of CVPR 2019 Habitat PointGoal Navigation Challenge. 
                </td>
            </tr>

			<!-- CoRL2019 -->
			<tr>
                    <td width="25%" align="center">
                        <a href="https://arxiv.org/pdf/1910.03568.pdf" target="_blank"><img class="centered-and-cropped"  src="./dhiraj gandhi_files/0_corl-ocmpc.gif" width="100%" alt="GIF" /></a>
                    </td>
                    <td width="75%" valign="top">
                        <p>
                            <a href="https://judyye.github.io/ocmpc/" target="_blank">
                                <heading>Object-centric Forward Modeling for Model Predictive Control</heading>
                            </a>
                            <br>
                            <a href="https://judyye.github.io/" target="_blank">Yufei Ye</a> , <strong>Dhiraj Gandhi</strong>, <a href="http://www.cs.cmu.edu/~abhinavg/" target="_blank">Abhinav Gupta</a>, <a href="https://shubhtuls.github.io/" target="_blank">Shubham Tulsiani</a>
                            <br>
                            <strong>CoRL 2019</strong><br>
                            <a href="https://arxiv.org/pdf/1910.03568.pdf" target="_blank">arXiv</a>, <a href="https://judyye.github.io/ocmpc/" target="_blank">project page</a>
                        </p>
                        <p align="justify"> In this paper we present an approach to learn an object-centric forward model, and show that this allows us to plan for sequences of actions to achieve distant desired goals. We propose to model a scene as a collection of objects, each with an explicit spatial location and implicit visual feature, and learn to model the effects of actions using random interaction data. 
                    </td>
                </tr>

			<!-- PyRobot -->
			<tr>
                        <td width="25%" align="center">
                            <a href="https://arxiv.org/abs/1906.08236" target="_blank"><img class="centered-and-cropped"  src="https://thumbs.gfycat.com/DownrightDearIndianabat-size_restricted.gif" width="100%" alt="GIF" /></a>
                        </td>
                        <td width="75%" valign="top">
                            <p>
                                <a href="https://www.pyrobot.org/" target="_blank">
                                    <heading>PyRobot: An Open-source Robotics Framework for Research and Benchmarking</heading>
                                </a>
                                <br>
                                <a href="http://www.adithyamurali.com" target="_blank">Adithya Murali*</a>,<a href="http://www.adithyamurali.com" target="_blank">Adithya Murali*</a>, <a href="https://www.ri.cmu.edu/ri-people/kalyan-vasudev-alwala/" target="_blank">Kalyan Vasudev Alwala*</a>, <strong>Dhiraj Gandhi*</strong>, <a href="http://www.cs.cmu.edu/~lerrelp/" target="_blank">Lerrel Pinto</a>, <a href="http://saurabhg.web.illinois.edu/" target="_blank">Saurabh Gupta</a>, <a href="http://www.cs.cmu.edu/~abhinavg/" target="_blank">Abhinav Gupta</a> [* Equal contribution]
                                <br>
                                <a href="https://arxiv.org/abs/1906.08236" target="_blank">arXiv</a>, <a href="https://www.pyrobot.org/" target="_blank">project page</a>, <a href="https://github.com/facebookresearch/pyrobot" target="_blank">code</a>, <a href="https://ai.facebook.com/blog/open-sourcing-pyrobot-to-accelerate-ai-robotics-research/" target="_blank">facebook AI blog</a>
                            </p>
                            <p align="justify">This paper introduces PyRobot, an open-source robotics framework for research and benchmarking. PyRobot is a light-weight, high-level interface on top of ROS that provides a consistent set of hardware independent mid-level APIs to control different robots. PyRobot abstracts away details about low-level controllers and inter-process communication, and allows non-robotics researchers (ML, CV researchers) to focus on building high-level AI applications.
                        </td>
                    </tr>

                        <!-- ICML 2019 -->
                        <tbody>
                            <tr>
                                <td width="25%" valign="top">
                                    <div style="width: 200px; overflow: hidden">
                                    <a href="https://arxiv.org/pdf/1906.04161.pdf"><img class="centered-and-cropped"  src="https://people.eecs.berkeley.edu/~pathak/images/icml19.gif" width='200' alt="GIF" /></a>
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/pdf/1906.04161.pdf">
                                            <heading>Self-Supervised Exploration via Disagreement</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <a href="https://people.eecs.berkeley.edu/~pathak/">Deepak Pathak*</a>, <strong>Dhiraj Gandhi*</strong> ,<a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a> [* Equal contribution]<br>
                                        <strong>ICML 2019</strong><br>
                                        <a href="https://arxiv.org/pdf/1906.04161.pdf">arXiv</a>, <a href="https://pathak22.github.io/exploration-by-disagreement/">project page</a>, <a href="https://github.com/pathak22/exploration-by-disagreement">code</a> 

                                    <p align="justify">In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward.
                                </td>
                            </tr>
                        </tbody>

                        <!-- Robots in Home -->
                        <tbody>
                            <tr>
                                <td width="25%" valign="top">
                                    <div style="width: 200px; overflow: hidden">
                                    <a href="https://arxiv.org/abs/1807.07049"><img class="centered-and-cropped"  src="https://thumbs.gfycat.com/DangerousAnnualAsianelephant-size_restricted.gif" width='200' alt="GIF" /></a>
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/1807.07049">
                                            <heading>Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta*</a>, <a href="http://www.adithyamurali.com">Adithya Murali*</a>, <strong>Dhiraj Gandhi*</strong> , <a href="http://www.cs.cmu.edu/~lerrelp/">Lerrel Pinto*</a> [*Equal contribution]<br>
                                        <strong>NIPS2018</strong><br>
                                        <a href="https://arxiv.org/abs/1807.07049">arXiv</a>, <a href="https://github.com/lerrel/home_dataset">data-set</a>
                                    <p>
					We present the first systematic effort in collecting a large dataset for robotic grasping in homes. The models trained with our home dataset showed a marked improvement of 43.7% over a baseline model trained with data collected in lab. Our architecture which explicitly models the latent noise in the dataset also performed 10% better than one that did not factor out the noise.
<!--First, to scale and parallelize data collection, we built a low cost mobile manipulator assembled for under 3K USD. Second, data collected using low cost robots suffer from noisy labels due to imperfect execution and calibration errors. To handle this, we develop a framework which factors out the noise as a latent variable. Our model is trained on 28K grasps collected in several houses under an array of different environmental conditions. The models trained with our home dataset showed a marked improvement of 43.7% over a baseline model trained with data collected in lab. Our architecture which explicitly models the latent noise in the dataset also performed 10% better than one that did not factor out the noise.-->
				    </p>
				    <br><br>
                                </td>
                            </tr>
                        </tbody>

                        <!--Learning to Grasp Without Seeing -->
                        <tbody>
                        <tr>
                            <td width="25%">
                            <a href="https://www.youtube.com/watch?v=oDTSfLUcXZ8"><img class="centered-and-cropped"  src="https://thumbs.gfycat.com/AlarmedCautiousGlassfrog-size_restricted.gif" width='200' height="150" alt="GIF" /></a>
                            </td>
                            <td width="75%" valign="top">
                                <p>
                                    <a href="https://arxiv.org/abs/1805.04201">
                                        <heading>Learning to Grasp Without Seeing</heading>
                                    </a>
                                    <br>
                                    <a href="http://adithyamurali.com/">Adithyavairavan Murali</a>, <a href="http://yinli.cvpr.net/">Yin Li</a>, <strong>Dhiraj Gandhi</strong>, <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a><br>
                                    <strong>ISER2018</strong><br>
                                    <a href="https://arxiv.org/abs/1805.04201">arXiv</a>, <a href="https://www.youtube.com/watch?v=oDTSfLUcXZ8">video</a>, <a href="https://cmu.app.box.com/s/rx8pgq0ti4ua2vxl6gsc7s97eorsi4a4">dataset</a>, <a href="http://www.cs.cmu.edu/afs/cs/user/amurali/www/projects/GraspingWithoutSeeing/#data">project page</a>
                                </p><p>
                                Can a robot grasp an unknown object without seeing it? In this paper, we present a tactile-sensing based approach to this challenging problem of grasping novel objects without prior knowledge of their location or physical properties. Our key idea is to combine touch based object localization with tactile based re-grasping. To train our learning models, we created a large-scale grasping dataset, including more than 30 RGB frames and over 2.8 million tactile samples from 7800 grasp interactions of 52 objects. 
            </p>
                <br><br>
                            </td>
                        </tr>
                    </tbody>

                        <!-- curriculum learning -->
                        <tbody>
                        <tr>
                            <td width="25%">
                                <a href="https://www.youtube.com/watch?v=iCQsM7EE4HI"><img class="centered-and-cropped" src="https://thumbs.gfycat.com/IndolentImprobableBittern-size_restricted.gif" width="200" height="150" alt="GIF"></a>
                            </td>
                            <td width="75%" valign="top">
                                <p>
                                    <a href="https://arxiv.org/pdf/1708.01354.pdf">
                                        <heading>CASSL: Curriculum Accelerated Self-Supervised Learning</heading>
                                    </a>
                                    <br>
                                    <a href="http://adithyamurali.com/">Adithyavairavan Murali</a>, <a href="http://www.cs.cmu.edu/~lerrelp/">Lerrel Pinto</a>, <strong>Dhiraj Gandhi</strong>, <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a><br>
                                    <strong>ICRA, 2018</strong><br>
                                    <a href="https://arxiv.org/pdf/1708.01354.pdf">arXiv</a>, <a href="https://www.youtube.com/watch?v=iCQsM7EE4HI">video</a> 
                                </p><p>
                                Recent self-supervised learning approaches focus on using a few thousand data points to learn policies for high-level, low-dimensional action spaces. However, scaling this framework for high-dimensional control require either scaling up the data collection efforts or using a clever sampling strategy for training. We present a novel approach - Curriculum Accelerated Self-Supervised Learning (CASSL) - to train policies that map visual information to high-level, higher- dimensional action spaces. 
            </p>
                <br><br>
                            </td>
                        </tr>
                    </tbody>

                        <!-- learning to fly by crashing -->
                        <tbody>
                            <tr>
                                <td width="25%">
                                    <div style="width: 200px; overflow: hidden">
                                    <a href="https://www.youtube.com/watch?v=HbHqC8HimoI&amp;feature=youtu.be"><img class="centered-and-cropped" src="https://thumbs.gfycat.com/ReliableJadedBufflehead-size_restricted.gif" width="200" alt="GIF"></a>
                                    <div style="width: 200px; overflow: hidden">
					<br>
                                    <a href="https://www.youtube.com/watch?v=HbHqC8HimoI&amp;feature=youtu.be"><img class="centered-and-cropped" src="https://thumbs.gfycat.com/SomberForcefulAnnelid-size_restricted.gif" width="200" alt="GIF"></a>
                                </div></div></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="http://arxiv.org/abs/1704.05588">
                                            <heading>Learning to Fly by Crashing</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <strong>Dhiraj Gandhi</strong>, <a href="http://www.cs.cmu.edu/~lerrelp/">Lerrel Pinto</a> , <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a><br>
                                        <strong>IROS, 2017</strong><br>
                                        <a href="http://arxiv.org/abs/1704.05588">arXiv</a>, <a href="https://www.youtube.com/watch?v=HbHqC8HimoI&amp;feature=youtu.be">video</a>, <a href="https://drive.google.com/drive/folders/0B1fqtmnleDX5VXBYR0o4SVdiWnc?usp=sharing">dataset & trained model</a> 
                                        
                                    </p><p>How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid obstacles? In this project, we propose to bite the bullet and collect a dataset of crashes itself! We build a drone whose sole purpose is to crash into objects: it samples naive trajectories and crashes into random objects. We crash our drone around 11K times to create one of the biggest UAV crash dataset. We show that simple self-supervised models learnt on this data is quite effective in navigating the UAV even in extremely cluttered environments with dynamic obstacles including humans.</p>
				    <br><br>
                                </td>
                            </tr>
                        </tbody>

                        <!-- CURIOUS ROBOT -->
						<tbody>
                            <tr>
                                <td width="25%"><a href="./dhiraj gandhi_files/teaser_curious_robot.png"><img src="./dhiraj gandhi_files/teaser_curious_robot.png" alt="teaser_curious_robot" width="200" style="border-style: none"></a><br><br>
                                    <a href="./dhiraj gandhi_files/NN_curious_robot.png"><img src="./dhiraj gandhi_files/NN_curious_robot.png" width="200" style="border-style: none"></a>
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/1604.01360">
                                            <heading>The Curious Robot: Learning Visual Representations via Physical Interactions</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <a href="http://www.cs.cmu.edu/~lerrelp/">Lerrel Pinto</a>, <strong>Dhiraj Gandhi</strong>, <a href="https://sites.google.com/site/yuanfengrickhan/">Yuanfeng Han</a>, <a href="http://www.cs.cmu.edu/~ylpark/">Yong-Lae Park</a> and <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a><br>
                                        <font color="red"><b>Spotlight Presentation</b></font> at <b>ECCV 2016</b><br>
                                        <a href="https://arxiv.org/pdf/1604.01360v2">arXiv</a> 
                                    </p><p>What is the right supervisory signal to train visual representations? In case of biological agents, visual representation learning does not require semantic labels. In fact, we argue that biological agents use active exploration and physical interactions with the world to learn visual representations unlike current vision systems which just use passive observations (images and videos downloaded from web). Towards this goal, we build one of the first systems on a Baxter platform that pushes, pokes, grasps and actively observes objects in a tabletop environment. </p>
                                    <p>
                                        We use these physical interactions to collect more than 130K datapoints, with each datapoint providing backprops to a shared ConvNet architecture allowing us to learn visual representations. We show the quality of learned representations by observing neuron activations and performing nearest neighbor retrieval on this learned representation. Finally we evaluate our learned ConvNet on different image classification tasks.
                                        <br><br>
                                </p></td>
                            </tr>
                        </tbody>

                        <!-- The Hitech Robotic Systemz -->
                        <tbody>
                            <tr>
                                <td width="25%"> 
                                    <a href="https://drive.google.com/file/d/16e0p9bJwuUR8vyRUEpRNijCTbm_UW7Yw/view?usp=sharing"><img class="centered-and-cropped"  src="https://thumbs.gfycat.com/JampackedWhisperedAssassinbug-size_restricted.gif" width="200" alt="GIF" /></a> 
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://drive.google.com/file/d/16e0p9bJwuUR8vyRUEpRNijCTbm_UW7Yw/view?usp=sharing">
                                            <heading>Auto PID tuning of Automated Guided Vehicle(AGV)</heading>
                                        </a>
                                        <!--</a>--><br>
                                        Work experience at <a href="http://www.hitechroboticsystemz.com/">The Hi-Tech Robotic Systemz Ltd. </a><br> 
                                        <a href="https://drive.google.com/file/d/16e0p9bJwuUR8vyRUEpRNijCTbm_UW7Yw/view?usp=sharing">video</a> 
  
                                    </p> 
                                    <p>Engineers at company faced difficulties in tuning the AGVs PID controller using the prevalent <a href="https://staff.guilan.ac.ir/staff/users/chaibakhsh/fckeditor_repo/file/documents/Optimum%20Settings%20for%20Automatic%20Controllers%20(Ziegler%20and%20Nichols,%201942).pdf">Ziegler-Nichols technique</a>.  This tuning technique was also not robust enough to work on varying loading conditions (from 0-2000lb). To overcome this cumbersome process, I undertook an initiative to automate it. After conducting a literature review and running proof-of-concept experiments in the Gazebo simulator, I implemented a
                                    model free <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=BqaS_84AAAAJ&citation_for_view=BqaS_84AAAAJ:bnK-pcrLprsC">Iterative Feedback Tuning scheme</a>. AGVs tuned with this method are running smoothly in production for the past couple of years at the Honda and Nokia plants in India.
                                    </p>


                                </td>
                            </tr>
                        </tbody>


                        <!-- Path Planning -->
						<tbody>
                            <tr>
                                <td width="25%"> 
                                    <a href="https://youtu.be/E_MC7vWb62A"><img class="centered-and-cropped"  src="https://thumbs.gfycat.com/UncommonPartialKakarikis-size_restricted.gif" width="200" alt="GIF" /></a> 
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://youtu.be/E_MC7vWb62A">
                                            <heading>Sampling based path planning algorithm</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <a href="https://youtu.be/E_MC7vWb62A">video</a>, <a href="https://github.com/Dhiraj100892/Path-Planning">code</a> 
  
                                    </p> 
                                    <p> In this project I tried to study different sampling based algorithms like PRM, RRT, RRT* and implemented them in matlab. I tried to smooth out trajectory obtained using these algorithms using Bezier curve method. Furthermore I tried to exted these algoritms for car like non-holonomic object using Dubins curve and Reeds-Shepps curve.</p>
                                </td>
                            </tr>
                        </tbody>

                        <!-- Legged and wheeled robot -->
						<tbody>
                            <tr>
                                <td width="25%"> 
                                    <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDROEZIdklsRHFsc1E/view?usp=sharing"><img class="centered-and-cropped"  src="https://thumbs.gfycat.com/PerkySlushyBrownbutterfly-size_restricted.gif" width="200" height="150" alt="GIF" /></a>

                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDRTzlQVkxIZWMtb1U/view?usp=sharing">
                                            <heading>Design & Development of hybrid legged and wheeled robot</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <strong>Dhiraj Gandhi</strong>, <a href="https://sites.google.com/site/jagjeetsingh311010326/">Jagjeet Singh </a> , <a href="http://www.iitg.ac.in/annem/">A. Narayana Reddy</a><br>
                                        <em>Proceedings of <a href="http://www.iitk.ac.in/inacomm15/">iNaCoMM2015</a> </em><br>
                                        <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDROEZIdklsRHFsc1E/view?usp=sharing">pdf</a>, <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDRTzlQVkxIZWMtb1U/view?usp=sharing">video</a> 
  
                                    </p> 
                                    <p>Wheeled locomotion is most widely used mechanism for mobile robots on even terrain whereas walking
                                    mechanism is suitable for mobility of robots on uneven terrain. To achieve advantages of
                                    wheeled as well as biped robot, we designed a robot with both mobility mechanisms. A 6-degree of
                                    freedom (DoF) biped robot has been constructed with 3 DoF on each leg with mechanism of transforming it into a wheeled robot. 
                                    </p>

                                </td>
                            </tr>
                        </tbody>

                        <!-- Systemantics Intern -->
						<tbody>
                            <tr>
                                <td width="25%"> 
                                    <a href="./dhiraj gandhi_files/ppm.png"><img src="./dhiraj gandhi_files/ppm.png" width="200"  style="border-style: none"></a>
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDRamFsam11QVVsaWc/view?usp=sharing">
                                            <heading>Design of Parrlel Planar Manipulator (PPM)</heading>
                                        </a>
                                        <!--</a>--><br>
                                        2014 summer internship at <a href="http://www.systemantics.com/">Systemantics</a><br> 
                                        
                                        <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDRamFsam11QVVsaWc/view?usp=sharing">pdf</a>, <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDRNDBIcE5KMUs2Q1k/view">video</a> 
  
                                    </p> 
                                    <p>Parallel manipulators have higher payload capacity, higher mechanical rigidity and better
                                    accuracy than their serial counter parts. However presence of singularities within the workspace is major problem with these manipulators. In this project I worked on optimizing the ratio of singularity free workspace to the total space occupied by the mechanism with respect to link lenght. Furthermore I extended the work to come up with design algorithm that  engineers can use for its optimal design. On application side I used 2RR PPM to deposit glue
                                    evenly on engine gaskit in simulation</p>

                                </td>
                            </tr>
                        </tbody>

                        <!-- SPHERICAL ROBOT -->
                        <tbody>
                            <tr>
                                <td width="25%">
                                    <div style="width: 200px; height: 150px; overflow: hidden">
                                    <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDRWDIwMHE3ejhNeEU/view?usp=sharing"><img class="centered-and-cropped" src="https://thumbs.gfycat.com/PlayfulSereneEstuarinecrocodile-size_restricted.gif" width="200" height="150" alt="GIF"></a>
                                </div></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDRUTNCVlZHNlBzelk/view?usp=sharing">
                                            <heading>Design & Development of Spherical Robot</heading>
                                        </a>
                                        <!--</a>--><br>
                                        <strong>Dhiraj Gandhi</strong>, Akshay Khatri and <a href="http://www.sc.iitb.ac.in/~leena/">Leena Vachhani</a><br>
                                        2013 Summer internship at <a href="http://www.sc.iitb.ac.in/">SysCon Dept. IIT Bombay </a><br> 
                                        <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDRUTNCVlZHNlBzelk/view?usp=sharing">pdf</a>, <a href="https://drive.google.com/file/d/0B1Jn7RtPbkDRWDIwMHE3ejhNeEU/view?usp=sharing">video</a>  
  
                                    </p><p>Spherical Robots have several advantages over robots that use wheels or legs for locomotion,
                                    including inner component protection, low rolling resistance, and ability to move in any direction.
                                    At the
                                    same time, it brings number of challenging problems in modelling, stabilization, and path following.
                                    In this project we designed and manufactured novel two pendulum based Spherical robot within diameter of 6 inch and achieved remote controlled omni-directional movements. We also mathematically modeled the system to observe the effect of pendulum state on the motion of robot.
					<br><br>
                                </p></td>
                            </tr>
                        </tbody>

                        </table><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                            <tbody>
                                <tr>
                                    <td>
                                        <br>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                        </td>
                        </tr>
                        </tbody>
                    </table>
    
<a href="https://people.eecs.berkeley.edu/~barron/">Source stolen from here</a>

</body></html>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122317495-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-122317495-1');
</script>
